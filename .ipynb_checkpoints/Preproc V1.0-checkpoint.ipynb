{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ded7b80",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4d4b01ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hamma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\hamma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hamma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "import nltk\n",
    "from nltk import pos_tag, RegexpParser, word_tokenize\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import ImageGrab\n",
    "import time\n",
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import fitz\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43c2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "95915c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogui\n",
      "  Downloading PyAutoGUI-0.9.54.tar.gz (61 kB)\n",
      "     ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/61.2 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/61.2 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/61.2 kB 131.3 kB/s eta 0:00:01\n",
      "     ------------------- ------------------ 30.7/61.2 kB 130.4 kB/s eta 0:00:01\n",
      "     ------------------------- ------------ 41.0/61.2 kB 178.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 61.2/61.2 kB 217.0 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pymsgbox (from pyautogui)\n",
      "  Downloading PyMsgBox-1.0.9.tar.gz (18 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pytweening>=1.0.4 (from pyautogui)\n",
      "  Downloading pytweening-1.0.7.tar.gz (168 kB)\n",
      "     ---------------------------------------- 0.0/168.2 kB ? eta -:--:--\n",
      "     --------- --------------------------- 41.0/168.2 kB 991.0 kB/s eta 0:00:01\n",
      "     ------------------------- ------------ 112.6/168.2 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 168.2/168.2 kB 1.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyscreeze>=0.1.21 (from pyautogui)\n",
      "  Downloading PyScreeze-0.1.29.tar.gz (25 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pygetwindow>=0.0.5 (from pyautogui)\n",
      "  Downloading PyGetWindow-0.0.9.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting mouseinfo (from pyautogui)\n",
      "  Downloading MouseInfo-0.1.3.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyrect (from pygetwindow>=0.0.5->pyautogui)\n",
      "  Downloading PyRect-0.2.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyscreenshot (from pyscreeze>=0.1.21->pyautogui)\n",
      "  Downloading pyscreenshot-3.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: Pillow>=9.3.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from pyscreeze>=0.1.21->pyautogui) (9.4.0)\n",
      "Collecting pyperclip (from mouseinfo->pyautogui)\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting EasyProcess (from pyscreenshot->pyscreeze>=0.1.21->pyautogui)\n",
      "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
      "Collecting entrypoint2 (from pyscreenshot->pyscreeze>=0.1.21->pyautogui)\n",
      "  Downloading entrypoint2-1.1-py2.py3-none-any.whl (9.9 kB)\n",
      "Collecting mss (from pyscreenshot->pyscreeze>=0.1.21->pyautogui)\n",
      "  Downloading mss-9.0.1-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: pyautogui, pygetwindow, pyscreeze, pytweening, mouseinfo, pymsgbox, pyperclip, pyrect\n",
      "  Building wheel for pyautogui (pyproject.toml): started\n",
      "  Building wheel for pyautogui (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyautogui: filename=PyAutoGUI-0.9.54-py3-none-any.whl size=37597 sha256=ca53a901cea43081f11c60a8d5e8ec05392546192176e1c891f6a79a10fa0a2d\n",
      "  Stored in directory: c:\\users\\hamma\\appdata\\local\\pip\\cache\\wheels\\95\\dc\\b1\\fe122b791e0db8bf439a0e6e1d2628e48f10bf430cae13521b\n",
      "  Building wheel for pygetwindow (setup.py): started\n",
      "  Building wheel for pygetwindow (setup.py): finished with status 'done'\n",
      "  Created wheel for pygetwindow: filename=PyGetWindow-0.0.9-py3-none-any.whl size=11079 sha256=df76b57c560dfb8a2d1b1eb18f9914318c6575b571bc2f4ce5b5e62bcca4ce19\n",
      "  Stored in directory: c:\\users\\hamma\\appdata\\local\\pip\\cache\\wheels\\07\\75\\0b\\7ca0b598eb4c21d43ba4bcc78a0538dfcf803a5997da33bc19\n",
      "  Building wheel for pyscreeze (pyproject.toml): started\n",
      "  Building wheel for pyscreeze (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyscreeze: filename=PyScreeze-0.1.29-py3-none-any.whl size=13484 sha256=e44b34715f6a95ef26068c6b2efc73859b939528cd0dec70f23f29fcbcf74488\n",
      "  Stored in directory: c:\\users\\hamma\\appdata\\local\\pip\\cache\\wheels\\d3\\bd\\06\\4b4c57f65e89d1ab7a63a924c12aca4784cd95e74940371b5e\n",
      "  Building wheel for pytweening (setup.py): started\n",
      "  Building wheel for pytweening (setup.py): finished with status 'done'\n",
      "  Created wheel for pytweening: filename=pytweening-1.0.7-py3-none-any.whl size=6214 sha256=434e7dd9b38448d61189321b5357b4ac543a670fc9058271bf8cb56c52915224\n",
      "  Stored in directory: c:\\users\\hamma\\appdata\\local\\pip\\cache\\wheels\\b2\\9b\\02\\059beba389e7e31a635bd9e8d9b7299f4ec11caca1f237f56d\n",
      "  Building wheel for mouseinfo (setup.py): started\n",
      "  Building wheel for mouseinfo (setup.py): finished with status 'done'\n",
      "  Created wheel for mouseinfo: filename=MouseInfo-0.1.3-py3-none-any.whl size=10906 sha256=3c2ad38787787a267e95e6bae45576a7c0530ff7b32d65e307643513620a29d4\n",
      "  Stored in directory: c:\\users\\hamma\\appdata\\local\\pip\\cache\\wheels\\20\\0b\\7f\\939ac9ff785b09951c706150537572c00123412f260a6024f3\n",
      "  Building wheel for pymsgbox (pyproject.toml): started\n",
      "  Building wheel for pymsgbox (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pymsgbox: filename=PyMsgBox-1.0.9-py3-none-any.whl size=7417 sha256=5f0cbebe7455a6f7658ddab35caf285dd3801b833ce32f5c59a0df3bd54cc884\n",
      "  Stored in directory: c:\\users\\hamma\\appdata\\local\\pip\\cache\\wheels\\85\\92\\63\\e126ee5f33d8f2ed04f96e43ef5df7270a2f331848752e8662\n",
      "  Building wheel for pyperclip (setup.py): started\n",
      "  Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=07fbe8f1d2b0c29b5c72fcb52d3cd7dd6fece928e0911b224836973c1a88e7e2\n",
      "  Stored in directory: c:\\users\\hamma\\appdata\\local\\pip\\cache\\wheels\\70\\bd\\ba\\8ae5c080c895c9360fe6e153acda2dee82527374467eae061b\n",
      "  Building wheel for pyrect (setup.py): started\n",
      "  Building wheel for pyrect (setup.py): finished with status 'done'\n",
      "  Created wheel for pyrect: filename=PyRect-0.2.0-py2.py3-none-any.whl size=11205 sha256=0773621ad9546ba940e01ce915bd4de8530b5737261fe9390f52a205a3e1dd34\n",
      "  Stored in directory: c:\\users\\hamma\\appdata\\local\\pip\\cache\\wheels\\c4\\e9\\fc\\b7a666dd4f9a3168fb44d643079b41d36ddab52f470707e820\n",
      "Successfully built pyautogui pygetwindow pyscreeze pytweening mouseinfo pymsgbox pyperclip pyrect\n",
      "Installing collected packages: pytweening, pyrect, pyperclip, pymsgbox, entrypoint2, EasyProcess, pygetwindow, mss, mouseinfo, pyscreenshot, pyscreeze, pyautogui\n",
      "Successfully installed EasyProcess-1.1 entrypoint2-1.1 mouseinfo-0.1.3 mss-9.0.1 pyautogui-0.9.54 pygetwindow-0.0.9 pymsgbox-1.0.9 pyperclip-1.8.2 pyrect-0.2.0 pyscreenshot-3.1 pyscreeze-0.1.29 pytweening-1.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "efd21c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f42a4",
   "metadata": {},
   "source": [
    "# Transforming to pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5620988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3b192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF file paths\n",
    "pdfPMBOK = \"PMBOK6-2017.pdf\" #431-494\n",
    "pdfPMI = \"practice-standard-project-risk-management.pdf\" #1-116\n",
    "output_folder_1=\"PMBOOKimgs\"\n",
    "output_folder_2=\"Practiceimgs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a3c8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_images(pdf_file_path, output_folder, start_page, end_page):\n",
    "    images = convert_from_path(pdf_file_path, output_folder=output_folder, fmt='png', first_page=start_page, last_page=end_page, output_file=\"page\")\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "510e6200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "convert_pdf_to_images(pdfPMBOK, output_folder_1, 431, 494)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba9413fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "convert_pdf_to_images(pdfPMI, output_folder_2, 1, 116)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c82c69",
   "metadata": {},
   "source": [
    "# Extracting Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c07f4",
   "metadata": {},
   "source": [
    "### YOLO: (does not work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047340b",
   "metadata": {},
   "source": [
    "###### must install yolov3.weights yolov3.cfg coco.names (ask chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a2a64a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_object_detection(image_path):\n",
    "    # Load YOLO\n",
    "    net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "    classes = []\n",
    "    with open(\"coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    layer_names = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Preprocess image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(layer_names)\n",
    "\n",
    "    # Get class IDs, confidences, and bounding boxes\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            print(f\"Confidence: {confidence}\")\n",
    "            if confidence > 0.1 and class_id == 0:  # Class ID 0 is \"person\"\n",
    "                center_x, center_y, w, h = (detection[:4] * np.array([width, height, width, height])).astype(int)\n",
    "                x, y = int(center_x - w / 2), int(center_y - h / 2)\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, int(w), int(h)])\n",
    "\n",
    "    # Apply non-maximum suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Save screenshots for detected regions\n",
    "    print(indices)\n",
    "\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        x, y, w, h = box\n",
    "\n",
    "        # Take a screenshot of the detected region using pyautogui\n",
    "        screenshot = pyautogui.screenshot(region=(x, y, w, h))\n",
    "        screenshot.save(f\"screenshot_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "902d570f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yolo_object_detection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m yolo_object_detection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPMBOOKimgs/page0001-432.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yolo_object_detection' is not defined"
     ]
    }
   ],
   "source": [
    "yolo_object_detection(\"PMBOOKimgs/page0001-432.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659e520",
   "metadata": {},
   "source": [
    "### openCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc6b6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_shapes(image_path, min_contour_area=1000):\n",
    "    # Read the image\n",
    "    image = cv2.imread(\"PMBOOKimgs/\"+image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Use Canny edge detection\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    k=0\n",
    "    for contour in contours:\n",
    "        k=k+1\n",
    "        # Calculate the contour area\n",
    "        area = cv2.contourArea(contour)\n",
    "\n",
    "        # If the contour area is greater than the minimum threshold\n",
    "        if area >= min_contour_area:\n",
    "            # Approximate the contour as a polygon\n",
    "            epsilon = 0.04 * cv2.arcLength(contour, True)\n",
    "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "            # Check if the polygon has 4 vertices, indicating a rectangle\n",
    "            if len(approx) == 4:\n",
    "                # Get the bounding box coordinates of the rectangle\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "                # Define a region of interest (ROI) using the rectangle coordinates\n",
    "                roi = image[y:y+h, x:x+w]\n",
    "\n",
    "                # Save the ROI as an image for inspection\n",
    "                cv2.imwrite(\"screenshots/\"+image_path+\"/\"+str(k)+\"detected__\"+image_path, roi)                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a1628722",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_shapes(\"page0001-445.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6dd1e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in os.listdir(\"PMBOOKimgs/\"):\n",
    "    os.makedirs(\"screenshots/\"+k, exist_ok=True)\n",
    "    detect_shapes(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb36c9d",
   "metadata": {},
   "source": [
    "# Extracting Texts from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7dd27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\hamma\\AppData\\Local\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fc03f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3af30aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Use adaptive thresholding to segment text from the background\n",
    "    _, threshold = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Remove horizontal and vertical lines using morphological operations\n",
    "    kernel = np.ones((4, 4), np.uint8)\n",
    "    clean_image = cv2.morphologyEx(threshold, cv2.MORPH_CLOSE, kernel)\n",
    "    text = pytesseract.image_to_string(Image.fromarray(threshold))\n",
    "\n",
    "\n",
    "    print(text)\n",
    "    # Save the extracted text to a file\n",
    "    #with open(output_text_path, 'w', encoding='utf-8') as text_file:\n",
    "        #text_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11954218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project\n",
      "Management\n",
      "Plan\n",
      "\n",
      "Project management plan\n",
      "\n",
      "« Requirements management plan\n",
      "« Schedule management plan\n",
      "« Cost management plan\n",
      "\n",
      "« Resource management plan\n",
      "* Quality management plan\n",
      "« Risk management plan\n",
      "« Scope baseline\n",
      "\n",
      "* Schedule baseline\n",
      "\n",
      "* Cost baseline\n",
      "\n",
      "Project\n",
      "Documents\n",
      "\n",
      "reject documents\n",
      "Assumption log\n",
      "Cost estimates 11.2\n",
      "\n",
      "Issue log Risks\n",
      "Lessons learned register\n",
      "\n",
      "Project\n",
      "eceeeseserseseeseneces: Docurnents\n",
      "\n",
      "Risk register\n",
      "* Risk report\n",
      "\n",
      "Requirements documentation\n",
      "Resource requirements.\n",
      "Stakeholder register\n",
      "\n",
      "Pr\n",
      ".\n",
      ".\n",
      "* Duration estimates Identify\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "12.1\n",
      "Plan\n",
      "Procurement\n",
      "Management\n",
      "\n",
      "Procurement documentation\n",
      "\n",
      "12.2\n",
      "Conduct\n",
      "Procurements\n",
      "\n",
      "Agreements\n",
      "\n",
      "Enterprise/\n",
      "Organization\n",
      "\n",
      "Enterprise environmental factors.\n",
      "Organizational process assets\n",
      "\n",
      "Project documents updates\n",
      "« Assumption log\n",
      "\n",
      "* Issue log\n",
      "\n",
      "« Lessons learned register\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_text(\"screenshots/page0001-446.png/54detected__page0001-446.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78faee36",
   "metadata": {},
   "source": [
    "# Extracting Texts from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_text_and_figures(pdf_path, output_text_path, output_figures_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_output = []\n",
    "\n",
    "    for page_number in range(doc.page_count):\n",
    "        page = doc[page_number]\n",
    "        images = page.get_images(full=True)\n",
    "\n",
    "        for img_index, image in enumerate(images):\n",
    "            base_image = doc.extract_image(image)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_path = f\"figure_page{page_number + 1}_img{img_index + 1}.png\"\n",
    "\n",
    "            with open(image_path, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "\n",
    "            # OCR on the image\n",
    "            img_cv2 = cv2.imread(image_path)\n",
    "            gray = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2GRAY)\n",
    "            _, threshold = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "            text = pytesseract.image_to_string(Image.fromarray(threshold))\n",
    "\n",
    "            # Save the extracted text\n",
    "            text_output.append(f\"Page {page_number + 1}, Figure {img_index + 1}:\\n{text}\\n\")\n",
    "\n",
    "    # Save the extracted text to a file\n",
    "    with open(output_text_path, 'w', encoding='utf-8') as text_file:\n",
    "        text_file.writelines(text_output)\n",
    "\n",
    "    doc.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "220b0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "\n",
    "def extract_plain_text_from_pdf(pdf_file_path):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_file_path)\n",
    "\n",
    "    # Initialize an empty list to store the extracted data\n",
    "    data = []\n",
    "\n",
    "    for page_number in range(431, 495):\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        text = page.get_text()\n",
    "        blocks = text.split(\"\\n\")\n",
    "\n",
    "        # Initialize variables to store the current title and description\n",
    "        current_title = \"\"\n",
    "        current_description = \"\"\n",
    "\n",
    "        for block in blocks:\n",
    "            block = block.strip()\n",
    "\n",
    "            if \"Figure\" in block:\n",
    "                continue\n",
    "            elif block.isupper():\n",
    "                # If a new title is found, add the previous title and description to the data list\n",
    "                if current_title:\n",
    "                    data.append({\"title\": current_title, \"description\": current_description})\n",
    "                current_title = block\n",
    "                current_description = \"\"\n",
    "            elif block.istitle():\n",
    "                # If a new title is found, add the previous title and description to the data list\n",
    "                if current_title:\n",
    "                    data.append({\"title\": current_title, \"description\": current_description})\n",
    "                current_title = block\n",
    "                current_description = \"\"\n",
    "            elif block.startswith(\"- \"):\n",
    "                continue\n",
    "            else:\n",
    "                # Append the block to the current description\n",
    "                current_description += block + \"\\n\"\n",
    "\n",
    "        # Add the last title and description to the data list\n",
    "        if current_title:\n",
    "            data.append({\"title\": current_title, \"description\": current_description})\n",
    "\n",
    "    # Close the PDF document\n",
    "    pdf_document.close()\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1033f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract plain text and create DataFrame\n",
    "df = extract_plain_text_from_pdf(pdfPMBOK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b5d66c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Part 1 - Guide</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.1 Inputs</td>\n",
       "      <td>.1 Project charter\\n.2 Project management plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.2 Tools &amp; Techniques</td>\n",
       "      <td>.1 Expert judgment\\n.2 Data analysis\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.3 Meetings</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.3 Outputs</td>\n",
       "      <td>.1 Risk management plan\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>11.7.3.3 PROJECT MANAGEMENT PLAN UPDATES</td>\n",
       "      <td>Any change to the project management plan goes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Part 1 - Guide</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>11.7.3.4 PROJECT DOCUMENTS UPDATES</td>\n",
       "      <td>Project documents that may be updated as a res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>11.7.3.5 ORGANIZATIONAL PROCESS ASSETS UPDATES</td>\n",
       "      <td>Organizational process assets that are updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>PROJECT PROCUREMENT MANAGEMENT</td>\n",
       "      <td>Project Procurement Management includes the pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title  \\\n",
       "0                                    Part 1 - Guide   \n",
       "1                                         .1 Inputs   \n",
       "2                             .2 Tools & Techniques   \n",
       "3                                       .3 Meetings   \n",
       "4                                        .3 Outputs   \n",
       "..                                              ...   \n",
       "476        11.7.3.3 PROJECT MANAGEMENT PLAN UPDATES   \n",
       "477                                  Part 1 - Guide   \n",
       "478              11.7.3.4 PROJECT DOCUMENTS UPDATES   \n",
       "479  11.7.3.5 ORGANIZATIONAL PROCESS ASSETS UPDATES   \n",
       "480                  PROJECT PROCUREMENT MANAGEMENT   \n",
       "\n",
       "                                           description  \n",
       "0                                                       \n",
       "1    .1 Project charter\\n.2 Project management plan...  \n",
       "2               .1 Expert judgment\\n.2 Data analysis\\n  \n",
       "3                                                       \n",
       "4                            .1 Risk management plan\\n  \n",
       "..                                                 ...  \n",
       "476  Any change to the project management plan goes...  \n",
       "477                                                     \n",
       "478  Project documents that may be updated as a res...  \n",
       "479  Organizational process assets that are updated...  \n",
       "480  Project Procurement Management includes the pr...  \n",
       "\n",
       "[481 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28ae83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac18f7c",
   "metadata": {},
   "source": [
    "# Cleaning Data and Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00bdd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters and symbols (retain letters, numbers, and basic punctuation)\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9.,!? ]', '', text)\n",
    "    \n",
    "    # Remove extra spaces and trim leading/trailing spaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ecc22",
   "metadata": {},
   "source": [
    "#  Tokenization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3098f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18136c",
   "metadata": {},
   "source": [
    "#  POS-TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a7f72345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    # Tokenize the text into words\n",
    "    words = tokenize(text)\n",
    "\n",
    "    # Perform POS tagging\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d308651",
   "metadata": {},
   "source": [
    "#  STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2cf2924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_from_list(text):\n",
    "    \n",
    "    pos_tags=pos_tagging(text)\n",
    "    word_list=tokenize(text)\n",
    "    # Define a list of POS tags for words to keep (e.g., nouns and adjectives)\n",
    "    allowed_pos_tags = ['DT','NN','VB', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS']  # Nouns and adjectives\n",
    "    \n",
    "    # Filter out stopwords based on POS tags\n",
    "    filtered_words = [word for word, pos_tag in zip(word_list, pos_tags) if pos_tag[1]\n",
    "                      in allowed_pos_tags]\n",
    "    \n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa24a4e",
   "metadata": {},
   "source": [
    "#  Chunking :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "953f6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "def chunk_words_by_meaning(text):\n",
    "    # Tokenize the text into words\n",
    "    words = tokenize(text)\n",
    "\n",
    "    # Perform POS tagging\n",
    "    pos_tags = pos_tagging(text)\n",
    "\n",
    "    # Define a grammar for chunking (NP: Noun Phrase)\n",
    "    # Define a grammar for chunking\n",
    "    grammar = r\"\"\"\n",
    "        NP: {<IN|DT|JJ|PRP>*<NN.*>+}   # chunk determiners, adjectives, personal pronouns, and nouns\n",
    "        VP: {<VB.*>+<NP>?}      # chunk verb phrases with optional NP\n",
    "        CHUNK: {<NP><VP>+<NP>}  # chunk relation with NP-VP-NP pattern\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a chunk parser with the defined grammar\n",
    "    \n",
    "    chunk_parser = RegexpParser(grammar)\n",
    "\n",
    "    \n",
    "    # Apply chunking to POS-tagged words\n",
    "    chunks = chunk_parser.parse(pos_tags)\n",
    "\n",
    "    # Extract phrases from the tree\n",
    "    result = [' '.join([token for token, pos in subtree.leaves()]) if type(subtree) == nltk.Tree else subtree[0]\n",
    "              for subtree in chunks]\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee466d",
   "metadata": {},
   "source": [
    "#### after this phase it should be returned a list of chunks cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c331df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Risks will continue to emerge during the lifetime of the project, so Project Risk Management processes should \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "97c6ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=remove_stopwords_from_list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "79d4f3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Risks continue emerge the lifetime the project Project Risk Management processes'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "034a0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=chunk_words_by_meaning(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d89d2ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (CHUNK\n",
      "    (NP Risks/NNS)\n",
      "    (VP continue/VBP emerge/VB (NP the/DT lifetime/NN))\n",
      "    (NP\n",
      "      the/DT\n",
      "      project/NN\n",
      "      Project/NNP\n",
      "      Risk/NNP\n",
      "      Management/NNP\n",
      "      processes/NNS)))\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529d875",
   "metadata": {},
   "source": [
    "#  Relationship & Concepts :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f7a950c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter  # Import the Counter class\n",
    "def extrac_relations(chunked_text):\n",
    "        # Extract and print identified concepts and relationships\n",
    "    concepts = []\n",
    "    relationships = []\n",
    "\n",
    "    for subtree in chunked_text.subtrees():\n",
    "        if subtree.label() == 'NP':\n",
    "            concepts.append(\" \".join(word for word, pos in subtree.leaves()))\n",
    "        elif subtree.label() == 'VP':\n",
    "            relationships.append(\" \".join(word for word, pos in subtree.leaves()))\n",
    "\n",
    "    # Concept frequencies\n",
    "    concept_freq = Counter(concepts)\n",
    "    frequency_threshold = 2  # Concepts that occur at least 10 times\n",
    "    pertinent_concepts = [concept for concept in concepts if concept_freq[concept] >= frequency_threshold]\n",
    "\n",
    "    print(f\"Total Concepts: {len(concepts)}\")\n",
    "    print(f\"Total Pertinent Concepts: {len(pertinent_concepts)}\")\n",
    "    print(f\"Total Relationships: {len(relationships)}\")\n",
    "    print(\"================================================================\")\n",
    "    print(\"Concepts:\", concepts)\n",
    "    print(\"Relationships:\", relationships)\n",
    "    print(\"Pertinent Concepts :\", pertinent_concepts)\n",
    "    return concepts, pertinent_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "66f4462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Concepts: 32\n",
      "Total Pertinent Concepts: 9\n",
      "Total Relationships: 22\n",
      "================================================================\n",
      "Concepts: ['Risks', 'the lifetime', 'the project', 'Project Risk Management processes', 'Risk', 'project planning', 'the project strategy', 'Risk', 'the project', 'the project', 'track', 'emergent risks', 'order', 'risk', 'a particular project', 'the project team', 'level', 'risk exposure', 'pursuit', 'the project', 'measurable risk thresholds', 'the risk appetite', 'the organization', 'project stakeholders', 'Risk thresholds', 'the degree', 'acceptable variation', 'a project objective', 'the project team', 'the deﬁnitions', 'risk impact levels', 'the project']\n",
      "Relationships: ['continue', 'emerge', 'be conducted', 'is', 'shaping the project strategy', 'be monitored', 'managed', 'progresses', 'ensure', 'stays', 'are addressed', 'manage risk', 'needs', 'know', 'is', 'objectives', 'is deﬁned', 'reﬂect the risk appetite', 'are', 'stated', 'communicated', 'reﬂected']\n",
      "Pertinent Concepts : ['the project', 'Risk', 'Risk', 'the project', 'the project', 'the project team', 'the project', 'the project team', 'the project']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Risks',\n",
       "  'the lifetime',\n",
       "  'the project',\n",
       "  'Project Risk Management processes',\n",
       "  'Risk',\n",
       "  'project planning',\n",
       "  'the project strategy',\n",
       "  'Risk',\n",
       "  'the project',\n",
       "  'the project',\n",
       "  'track',\n",
       "  'emergent risks',\n",
       "  'order',\n",
       "  'risk',\n",
       "  'a particular project',\n",
       "  'the project team',\n",
       "  'level',\n",
       "  'risk exposure',\n",
       "  'pursuit',\n",
       "  'the project',\n",
       "  'measurable risk thresholds',\n",
       "  'the risk appetite',\n",
       "  'the organization',\n",
       "  'project stakeholders',\n",
       "  'Risk thresholds',\n",
       "  'the degree',\n",
       "  'acceptable variation',\n",
       "  'a project objective',\n",
       "  'the project team',\n",
       "  'the deﬁnitions',\n",
       "  'risk impact levels',\n",
       "  'the project'],)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrac_relations(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424cc835",
   "metadata": {},
   "source": [
    "#  Frequencies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb3f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
